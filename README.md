# Exam Difficulty Predictor

### Intelligent Question Complexity Analysis via Feature Engineering & XGBoost

[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-Supported-FF4B4B?style=flat-square&logo=streamlit&logoColor=white)](https://streamlit.io)
[![XGBoost](https://img.shields.io/badge/XGBoost-1.7%2B-0F9D58?style=flat-square&logo=xgboost&logoColor=white)](https://xgboost.readthedocs.io/)

[Overview](#-overview) Â· [Architecture](#-system-architecture) Â· [Quickstart](#-quickstart) Â· [Pipeline](#-ml-pipeline) Â· [Results](#-results) Â· [Application](#-application)

---

## ğŸ¯ Overview

Educators, instructional designers, and testing organizations spend countless manual hours evaluating the difficulty and quality of examination questions. A misjudged question can skew test results and inaccurately measure student proficiency.

The Exam Difficulty Predictor acts as an automated "first pass" quality assurance tool. It is an **end-to-end, production-structured ML pipeline** taking raw question text (including LaTeX and math symbols) and instantly predicting how difficult it will be for students.

It predicts both a continuous difficulty index (p-value from 0.0 to 1.0) and a categorical difficulty tier (Easy, Medium, Hard). The underlying models are built with **XGBoost** and rely on 25 extracted lexical, mathematical, and domain-specific features.

### Problem Statement

| Challenge                      | Scale                         |
| ------------------------------ | ----------------------------- |
| Manual verification bottleneck | Hours spent per exam          |
| Dataset size (Exam Dataset)    | 50,000 preprocessed questions |
| Evaluation targets             | Continuous (P-value) & Tiers  |
| Structural complexity          | Text, LaTeX, Math Operators   |

---

## ğŸ— System Architecture

The project functions across three main sectors: data processing, model building, and real-time application inference.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXAM DIFFICULTY PREDICTOR SYSTEM                                 â”‚
â”‚                         Intelligent Question Complexity Analysis                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                                    DATA SOURCE
                           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                             exam_dataset_50k_unclean.csv
                          (Question Text + Answers + Metadata)


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TRAINING PIPELINE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Data Loading     â”‚
        â”‚ clean_dataset.py   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Preprocessing               â”‚
        â”‚------------------------------------------â”‚
        â”‚ â€¢ Standardize NULLs (N/A, none, ?, --)   â”‚
        â”‚ â€¢ Strip whitespaces & casing normalizer  â”‚
        â”‚ â€¢ Deduplication                          â”‚
        â”‚ â€¢ Enforce domain constraints (0-100%)    â”‚
        â”‚ â€¢ Outlier capping (IQR method)           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Export Clean Data  â”‚
        â”‚ exam_50k_clean.csv â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          Machine Learning Models         â”‚
        â”‚           genai_project.ipynb            â”‚
        â”‚------------------------------------------â”‚
        â”‚ â€¢ XGBoost Regressor (Continuous P-Value) â”‚
        â”‚ â€¢ XGBoost Classifier (Easy/Med/Hard)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Model Persistence  â”‚
        â”‚ Save JSON & PKL    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•



â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• INFERENCE PIPELINE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—

        User Input (Streamlit UI via app.py)
        (Question Text + Answer Options + Tier)
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Load Saved Models  â”‚
        â”‚ xgb_models.json    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Feature Engineering (NLP)         â”‚
        â”‚          feature_extractor.py            â”‚
        â”‚------------------------------------------â”‚
        â”‚ â€¢ Lexical Stats (Word & Sentence counts) â”‚
        â”‚ â€¢ Math Ops & LaTeX Density               â”‚
        â”‚ â€¢ Vocabulary Richness                    â”‚
        â”‚ â€¢ Domain Terms (Algebra, Stats, Calc)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            Prediction Engine             â”‚
        â”‚------------------------------------------â”‚
        â”‚ â€¢ xgb.DMatrix(features)                  â”‚
        â”‚ â€¢ predict_proba()                        â”‚
        â”‚ â€¢ Heuristic Bias (Subject Tier Nudge)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Difficulty Metrics Output     â”‚
        â”‚------------------------------------â”‚
        â”‚ â€¢ Predicted Class (Easy/Med/Hard)  â”‚
        â”‚ â€¢ Difficulty Index (p-value)       â”‚
        â”‚ â€¢ Confidence / Probabilities Plot  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ï¿½ Repository Structure

```
genai-project/
â”œâ”€â”€ README.md                         # â† You are here
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”‚
â”œâ”€â”€ app.py                            # Streamlit web application frontend
â”œâ”€â”€ feature_extractor.py              # Extractor for 25 lexical and structural features
â”œâ”€â”€ clean_dataset.py                  # Script to preprocess and sanitize raw data
â”œâ”€â”€ clean_dataset.ipynb               # Jupyter Notebook version of the data cleaner
â”œâ”€â”€ genai_project.ipynb               # XGBoost model training and evaluation notebook
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ question_analysis.ipynb       # NLP analysis notebook on questions
â”‚   â””â”€â”€ visualize_results.ipynb       # Notebook dedicated to plotting analysis results
â”‚
â”œâ”€â”€ raw_dataset/
â”‚   â””â”€â”€ exam_dataset_50k_unclean.csv  # Original provided uncleaned data
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ exam_dataset_50k_cleaned.csv  # Cleaned dataset output by the pipeline
â”‚
â””â”€â”€ files/                            # Model persistence
    â”œâ”€â”€ xgb_reg_model_A.json          # Saved XGBoost Regression Model
    â”œâ”€â”€ xgb_clf_model_A.json          # Saved XGBoost Classification Model
    â””â”€â”€ pipeline.pkl                  # Label Encoder / Pipeline
```

---

## ğŸš€ Quickstart

### 1. Clone & enter the repo

```bash
git clone <your-repo-link>
cd genai-project
```

### 2. Set up environment

```bash
python3 -m venv venv
source venv/bin/activate          # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Add the dataset (Optional)

Place the dataset inside `raw_dataset/` and run the cleaning script using `clean_dataset.py` or `clean_dataset.ipynb`.

### 4. Train the model (Optional)

```bash
jupyter notebook genai_project.ipynb
# Run all cells â€” models save to files/
```

### 5. Launch the app

```bash
streamlit run app.py
# Opens at http://localhost:8501
```

---

## ğŸ”¬ ML Pipeline

### Stage 1 â€” Feature Extraction

The user inputs text, and `feature_extractor.py` scans for mathematical operators, word structures, and domain terms (Geometry, Calculus, Statistics) extracting 25 quantitative features.

### Stage 2 â€” Domain Heuristics

Applies weighted nudges based on user-provided academic tiers.

### Stage 3 â€” Machine Learning Models

| Model                  | Task                                                  |
| ---------------------- | ----------------------------------------------------- |
| **XGBoost Regressor**  | Continuous difficulty index prediction (p-value).     |
| **XGBoost Classifier** | Categorical tier classification (Easy, Medium, Hard). |

---

## ğŸ“Š Results

The system utilizes **XGBoost (Extreme Gradient Boosting)**, trained on a dataset of ~50,000 rows.

| Metric          | Regression                                     | Classification                                              |
| --------------- | ---------------------------------------------- | ----------------------------------------------------------- |
| **Performance** | RMSE: ~0.1245 <br> MAE: ~0.0982 <br> RÂ²: ~0.78 | Accuracy: ~84.5% <br> Precision: ~0.85 <br> F1-Score: ~0.83 |

---

## ğŸ•¹ï¸ Application

### Usage via Streamlit

The main interface runs locally via Streamlit. You can:

1. Copy-paste a question and its multiple-choice options.
2. Specify the subject tier (1â€“5) and misconception levels.
3. Automatically receive evaluated classification probabilities, text complexity data, and a regression difficulty score.

---

## ğŸš¦ Limitations

- **Language Support:** Currently, the system evaluates questions primarily in English due to the NLP dependency libraries (like `textstat` and NLTK). Multi-language evaluation would require a different vectorization pipeline.
- **Image/Graph Dependency:** The model cannot parse or comprehend questions that rely primarily on images, charts, or graphical data contexts.
- **Lexical Bias:** The heuristic approach assumes that longer, more complex sentences with more math operators dictate mathematical/academic difficulty. This can occasionally misclassify a very conceptually difficult short question as "Easy".

---

## ğŸš€ Future Work â€” Milestone 2 (Agentic AI)

Moving beyond static ML models, Milestone 2 will introduce an iterative, agentic approach evaluating the _conceptual rigor_ of questions.

Unlike the current lexical approach, the future Agentic framework will utilize:

- **LLM Reasoning Engines:** Leveraging Large Language Models (like Gemini/GPT) to solve the question step-by-step and measure the logical complexity required instead of just lexical bounds.
- **RAG for Context Base:** Using Retrieval-Augmented Generation to measure how a question aligns against educational standards (e.g., Curriculum).
- **Multi-Modal Processing:** Introducing Vision-Language Models to handle questions heavily reliant on graphs, geometry diagrams, and image-based data.
- **Iterative Feedback Loops:** An agentic workflow where the model acts as a "tutor," testing various difficulty assumptions and adjusting its final difficulty rating based on simulated student failure modes.

---
